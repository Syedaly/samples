import os
import pandas as pd
import boto3
import requests
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, count, when

# Initialize Spark session
spark = SparkSession.builder.appName("DataValidation").getOrCreate()

# Define directory pairs (metadata_base_dir and parquet_base_dir)
directory_pairs = [
    ("/var/s3fs/OT800/ETBG", "/var/s3fs_t/OT800/ETBG"),
    ("/var/s3fs/OT800/ETBG1", "/var/s3fs_t/OT800/ETBG1"),
    ("/var/s3fs/OT800/ETBG2", "/var/s3fs_t/OT800/ETBG2")
]

# Function to get the instance ID of the current EC2 instance
def get_instance_id():
    try:
        # Query the instance metadata service
        response = requests.get("http://169.254.169.254/latest/meta-data/instance-id", timeout=5)
        response.raise_for_status()  # Raise an exception for HTTP errors
        return response.text
    except requests.RequestException as e:
        print(f"Error retrieving instance ID: {e}")
        return None

# EC2 Configuration
ec2_client = boto3.client("ec2", region_name="your-region")  # Replace with your AWS region
instance_id = get_instance_id()  # Automatically detect the instance ID

if not instance_id:
    raise Exception("Failed to retrieve instance ID. Ensure this script is running on an EC2 instance.")

# Function to normalize data types
def normalize_data_type(dtype):
    # Convert to lowercase
    dtype = dtype.lower()
    
    # Map CHAR to string
    if dtype == "char":
        return "string"
    return dtype

# Function to read the metadata and extract column names and data types
def read_metadata(metadata_path):
    # Read the metadata file, skip the first 2 rows
    metadata_df = pd.read_csv(metadata_path, skiprows=2, header=None)
    
    # Extract only the relevant columns (COL_NAME and DATA_TYPE)
    metadata_df = metadata_df[[1, 2]]
    
    # Rename columns for clarity
    metadata_df.columns = ["COL_NAME", "DATA_TYPE"]
    
    # Normalize data types
    metadata_df["DATA_TYPE"] = metadata_df["DATA_TYPE"].apply(normalize_data_type)
    
    # Convert to a dictionary
    column_data = dict(zip(metadata_df["COL_NAME"], metadata_df["DATA_TYPE"]))
    
    return column_data

# Function to check for null values in a DataFrame
def check_null_values(df, file_path):
    null_counts = df.select([count(when(col(c).isNull(), c)).alias(c) for c in df.columns])
    null_counts_dict = null_counts.collect()[0].asDict()
    
    null_summary = []
    for col_name, null_count in null_counts_dict.items():
        if null_count > 0:
            null_summary.append(f"Column {col_name} has {null_count} null values.")
    
    if null_summary:
        print(f"Null values found in {file_path}:")
        print("\n".join(null_summary))
    else:
        print(f"No null values found in {file_path}.")
    
    return null_summary

# Function to check for duplicate rows in a DataFrame
def check_duplicates(df, file_path):
    duplicate_count = df.count() - df.dropDuplicates().count()
    
    if duplicate_count > 0:
        print(f"Found {duplicate_count} duplicate rows in {file_path}.")
    else:
        print(f"No duplicate rows found in {file_path}.")
    
    return duplicate_count

# Function to write validation result to a file
def write_validation_result(message, output_path):
    try:
        # Write the message to the output file
        with open(output_path, "w") as file:
            file.write(message)
        print(f"Validation result written to: {output_path}")
    except Exception as e:
        print(f"Error writing validation result to {output_path}: {e}")

# Function to shut down the EC2 instance
def shutdown_ec2_instance():
    try:
        print("Shutting down EC2 instance...")
        ec2_client.stop_instances(InstanceIds=[instance_id])  # Stop the instance
        # ec2_client.terminate_instances(InstanceIds=[instance_id])  # Use this to terminate instead of stop
        print("EC2 instance shutdown initiated.")
    except Exception as e:
        print(f"Error shutting down EC2 instance: {e}")

# Function to process a single directory pair
def process_directory_pair(metadata_base_dir, parquet_base_dir):
    # Track overall validation result
    all_matches = True
    mismatch_summary = []

    # Iterate over subdirectories dynamically for metadata files
    for root, dirs, files in os.walk(metadata_base_dir):
        # Look for any file ending with .metadata
        metadata_files = [f for f in files if f.endswith(".metadata")]
        
        for metadata_file in metadata_files:
            metadata_path = os.path.join(root, metadata_file)
            
            try:
                # Read metadata schema
                metadata_columns = read_metadata(metadata_path)

                # Look for .parquet files in corresponding parquet_base_dir subdirectory
                relative_dir = os.path.relpath(root, metadata_base_dir)  # Get relative directory path
                parquet_dir = os.path.join(parquet_base_dir, relative_dir)  # Construct corresponding parquet directory path
                
                if os.path.exists(parquet_dir):
                    parquet_files = [f for f in os.listdir(parquet_dir) if f.endswith(".parquet")]
                    for parquet_file in parquet_files:
                        parquet_path = os.path.join(parquet_dir, parquet_file)
                        try:
                            df_parquet = spark.read.parquet(parquet_path)

                            # Extract schema from parquet
                            parquet_schema = df_parquet.schema
                            parquet_columns = [field.name for field in parquet_schema]
                            parquet_types = [field.dataType.simpleString().lower() for field in parquet_schema]  # Normalize to lowercase

                            # Store schema info
                            parquet_schema_dict = dict(zip(parquet_columns, parquet_types))

                            # Compare parquet schema with metadata schema
                            print(f"Comparing schema for {parquet_path}:")
                            file_mismatches = []

                            for col_name, dtype in parquet_schema_dict.items():
                                # Check if column exists in metadata
                                if col_name in metadata_columns:
                                    metadata_dtype = metadata_columns[col_name]
                                    if dtype != metadata_dtype:
                                        mismatch_message = f"Mismatch for column {col_name}: Parquet type is {dtype}, Metadata type is {metadata_dtype}"
                                        print(f"  {mismatch_message}")
                                        file_mismatches.append(mismatch_message)
                                        all_matches = False
                                    else:
                                        print(f"  Match for column {col_name}: Type is {dtype}")
                                else:
                                    mismatch_message = f"Column {col_name} not found in metadata!"
                                    print(f"  {mismatch_message}")
                                    file_mismatches.append(mismatch_message)
                                    all_matches = False

                            # Check for null values
                            null_summary = check_null_values(df_parquet, parquet_path)
                            file_mismatches.extend(null_summary)

                            # Check for duplicate rows
                            duplicate_count = check_duplicates(df_parquet, parquet_path)
                            if duplicate_count > 0:
                                file_mismatches.append(f"Found {duplicate_count} duplicate rows.")

                            if file_mismatches:
                                mismatch_summary.append(f"File: {parquet_path}\n" + "\n".join(file_mismatches))
                            else:
                                mismatch_summary.append(f"File: {parquet_path}\nAll columns match, no null values, and no duplicates.")

                            print("\n" + "-"*50 + "\n")
                        except Exception as e:
                            print(f"Error reading parquet file {parquet_path}: {e}")
            except Exception as e:
                print(f"Error reading metadata file {metadata_path}: {e}")

    # Prepare the message for this directory pair
    message = f"Validation results for directory pair:\nMetadata: {metadata_base_dir}\nParquet: {parquet_base_dir}\n\n"
    message += "\n".join(mismatch_summary)

    # Write the validation result to a file in the /var/s3fs/ directory
    output_dir = os.path.join("/var/s3fs/validation_results", os.path.basename(metadata_base_dir))
    os.makedirs(output_dir, exist_ok=True)  # Create the directory if it doesn't exist
    output_path = os.path.join(output_dir, "validation_result.txt")
    write_validation_result(message, output_path)

# Process each directory pair
for metadata_base_dir, parquet_base_dir in directory_pairs:
    print(f"Processing directory pair:\nMetadata: {metadata_base_dir}\nParquet: {parquet_base_dir}")
    process_directory_pair(metadata_base_dir, parquet_base_dir)

# Shut down the EC2 instance after processing
shutdown_ec2_instance()

# Stop Spark session
spark.stop()
