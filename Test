import json
import os
import re
from pyspark.sql.types import *
from pyspark.sql import functions as F
from datetime import datetime, date
import pandas as pd
from pyspark.sql import SparkSession


def get_data_from_gold_table(table_name, calculated_run_date):
    try:
        df_silver = spark.read.table(f"sap_gold_lh.{table_name}").where(F.to_date(F.col("data_last_created_timestamp")) == calculated_run_date)
        print(f"Successfully read {df_silver.count()} records from {table_name}")
        return df_silver
    except:
        raise(f"Table doen't exists")


print(f"Parameters - Scope: {scope}, Date: {calculated_run_date}")

##Hardcoding the values, future state would be metadata driven 
source_table_name = '0UCIODS16'
key_columns = 'UC_DEVICE,DATETO'
target_table = '0uc_device_q'


# Get source data
source_df = get_data_from_gold_table(source_table_name, calculated_run_date)
display(source_df)

# Get source record count 
print(f"Source recotd count : {source_df.count()}")

#Create temporary table
source_df.createOrReplaceTempView("source_data")

## Applying transformation logic to the source records
transformed_df = spark.sql("""SELECT 
                                LPAD(LTRIM(RTRIM(UC_DEVICE)), 18, '0') AS UC_DEVICE,
                                LPAD(LTRIM(RTRIM(FUNCT_LOC)), 30, '0') AS FUNCT_LOC,
                                FUNCT_LOC AS UC_DEVLOC,
                                LPAD(LTRIM(RTRIM(UC_WINDGRP)), 8, '0') AS UC_WINDGRP,
                                UCLOGICNR AS UCLOGICNR,
                                UCAMIMGATR AS UCAMIMGATR,
                                LPAD(LTRIM(RTRIM(UCAMCGDGR)), 10, '0') AS UCAMCGDGR,
                                UCAMIDGRP AS UCAMIDGRP,
                                ZINSTLYR AS ZINSTLYR,
                                UCAMCGCAPG AS UCAMCGCAPG,
                                UCREMOVDAT AS UCREMOVDAT,
                                UCINSTDATE AS UCINSTDATE,
                                LPAD(LTRIM(RTRIM(UC_REGGRP)), 8, '0') AS UC_REGGRP,
                                LPAD(LTRIM(RTRIM(UCAMS)), 4, '0') AS UCAMS,
                                DATEFROM AS DATEFROM,
                                UCACTREAS AS UCACTREAS,
                                DATETO AS DATETO,
                                UC_COMBDEV AS UC_COMBDEV,
                                from_utc_timestamp(current_timestamp(), 'PST') as data_last_created_timestamp,
                                from_utc_timestamp(current_timestamp(), 'PST') as data_last_updated_timestamp
                        FROM source_data"""
                        )

#Create temporary table
transformed_df.createOrReplaceTempView("source_data_transformed")

#display(transformed_df) 
source_column_list = transformed_df.columns 
select_column_list = ", ".join([col.lower() for col in source_column_list])
print(f"Source dataframe column list : {source_column_list}")

joining_columns = key_columns.split(',')

## Create joining condition
join_condition = "\n AND ".join([f"COALESCE(source.{col},'') = COALESCE(target.{col},'')" for col in joining_columns]) 

## get target table column details

if spark.catalog.tableExists(target_table):
    target_columns_df = spark.sql(f"DESCRIBE {target_table}")
    target_column_list = [row.col_name for row in target_columns_df.collect()]
else:
    ## Include create delta table logic
    print("Dummy - Needs to add creation of delta table logic")
print(f"Target table column list : {target_column_list}")

## Check if source columns are available in target table
missing_col_list =[]
# Find missing columns in target table
missing_col_list = [col for col in source_column_list if col not in target_column_list]

if missing_col_list:
    print(f"Some columns are not available in target table, please check the transformed dataframe alia  : {missing_col_list}")


# Build the MERGE statement using the temporary view with explicit column updates

update_column_list = set(source_column_list) - set(joining_columns) ## Removing the key columns from the update column list
update_column_list.discard('data_last_created_timestamp') ## Removing the created date audit columns from the update column list
update_sets = ",\n".join([f"target.{col} = source.{col}" for col in update_column_list])
print(f"Update condition : {update_sets}")



merge_components = {
        "base": f"""
            MERGE INTO {target_table} target
            USING source_data_transformed source
            ON {join_condition}
        """,
        "matched_update": f"""
            WHEN MATCHED THEN 
                UPDATE SET 
                    {update_sets}""",
        "not_matched": f"""
            WHEN NOT MATCHED THEN 
                INSERT ({', '.join(source_column_list)}) 
                VALUES ({select_column_list})"""
}

# Construct the final merge query
merge_query = f"""
        {merge_components['base']}
        {merge_components['matched_update']}
        {merge_components['not_matched']}
    """

print(f"Merge Query : {merge_query}")

#Execute Merge query

spark.sql(merge_query)


