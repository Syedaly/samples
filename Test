Mainframe Data Extraction and Migration Process

Overview

This document details the extraction and migration process of mainframe data, focusing on IMS and DB2 data structures. The process involves multiple stages, including extraction, filtering, reformatting, and migration to AWS. The workflow is primarily driven by JCL jobs and involves structured data transformation into a queryable format.

Data Extraction Process

IMS Extraction

Extracts are generated for OT-84, 500, and 800 datasets.

The extraction is performed by Jan Morrison’s team to feed into IDAA.

Each dataset consists of multiple structures (e.g., the 800 dataset has 113 different structures).

The extraction process generates separate files for each structure type.

The extraction is performed 10-wide, meaning 10 partitions run in parallel.

A job consolidates 10-wide partitions into one file per structure type.

DB2 Extraction

Unlike IMS, DB2 extracts were not available from Jan Morrison’s group.

The team developed their own DB2 extract process.

The extraction utilizes IBM DB2 Unload Utility.

Once extracted, the data follows the same process as IMS data (joining, filtering, reformatting, etc.).

Filtering and Data Transformation

Filter Process

The extracted data undergoes a filtering process to determine which records should be moved.

Three files are used for filtering:

Code Strip File

OX-3T File

Primary Structure File

These three files are joined together to create the filter file.

The filter file contains only the data required for migration.

Joining Process

The filter file is joined with the extracted data to retain only necessary records.

The output is stored in header files.

The header files serve as input for AWS reformatting.

Reformatting Process

The AWS reformatting process performs:

Binary and packed decimal conversions.

Data validation (e.g., replacing unacceptable characters).

Insertion of delimiters for compatibility.

The reformatted data is stored in structured output files.

Migration to AWS

Data Transfer to AWS

The reformatted files are transferred from TSDEV environment to AWS S3.

The data is stored in Parquet format after undergoing an ED conversion process.

The FTP process is used to transfer the datasets to AWS.

Metadata and Schema Handling

Chris's team creates metadata files.

The extracted data is joined with schemas before conversion to Parquet format.

The policy number is assigned as a key identifier to maintain relational integrity.

AWS Athena is used to create temporary views for SQL-based queries on Parquet files.

Partitioning and Archival Strategy

Partitioning is based on:

Database and segment type.

State and product type.

Termination date of policy records.

Non-policy-based datasets may require different partitioning strategies.

Data is archived based on retention policies (25-35 years based on termination date).

Challenges and Considerations

Schema Standardization

The schema for IMS and DB2 should be standardized.

Fields such as Policy Number (PLCY_NO) should be named consistently across datasets.

Queries should be designed to handle data sourced from both IMS and DB2.

Query Performance & Indexing

AWS Athena’s indexing and partitioning strategies need to be optimized.

SQL queries must be structured to avoid full table scans for performance efficiency.

Indexing strategies will be revisited based on consumer query requirements.

Data Verification & Testing

Data verification is essential before integration testing.

Sample datasets (20-policy extract) are provided for initial validation.

Integration testing will be conducted in TSDEV and AWS test environments.

Next Steps

Finalize Data Sample: Provide sample files (two segments minimum) for testing.

Schema Review: Validate field consistency between IMS and DB2 datasets.

AWS Query Testing: Validate Parquet format efficiency with Athena queries.

Integration Testing: Conduct AWS data validation with end-user queries.

Archive Planning: Define a structured archival and retention strategy.

Conclusion

The migration process follows a structured approach, ensuring that mainframe data is efficiently extracted, filtered, transformed, and migrated to AWS. The focus remains on query performance, schema standardization, and archival strategy to facilitate smooth data consumption and governance. Further collaboration will be required to refine partitioning and indexing strategies based on consumer needs.

