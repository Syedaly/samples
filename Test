**Overview**
This document presents an overview of the upstream systems used by the Auto Legacy Shutdown â€“ ILM to transfer data from the Mainframe to the AWS S3 Landing bucket. It also discusses the role of the Conversion Engine in transforming flat files into Parquet format before storing them in the S3 Raw bucket.

---

### AWS Transfer Family
AWS Transfer Family is a fully managed file transfer service that enables secure exchange of files using protocols such as SFTP. It plays a critical role in moving data between State Farm's on-premises mainframe and AWS S3. The following key aspects define its operation:

- Authentication occurs from State Farm On-Prem using SSH Key Pair.
- Supports bi-directional SFTP transfers using PUT or GET operations.
- CloudWatch logging is enabled for monitoring and issue tracking.
- The Transfer Family Endpoint Security Group enforces access restrictions via CIDR blocks.
- A dedicated Transfer Family User Role is created for secure S3 access in the AWS pcmngd04 tenant account.
- The User Role is also included in the S3 Bucket Policy and KMS Key Policy to manage permissions effectively.

#### Key Scenarios & Considerations
- **File Transfer Integrity:** If one file out of ten fails during transfer, SFTP rolls back the entire batch instead of partially transferring data.
- **File Size Management:** Though SFTP supports up to 5TB transfers, the EDE team recommends sending smaller files to prevent timeout errors.
- **Monitoring & Alerts:** Transfer attempts (both success and failure) are logged in CloudWatch, with dashboards available via Dynatrace and Splunk for ILM team access.
- **Network Dependencies:** Direct Connect is a PCAF module, and its health checks and outages are monitored by the PCAF team.

---

### DICE (Data Ingestion and Conformance Engine)
DICE is an internal service designed to automate AWS component creation for managing customer configurations. Initially, 60 Transfer Family customers were onboarded via Terraform; however, DICE now streamlines this process using Lambda functions.

#### Transfer Family Enablement via DICE
DICE automates Transfer Family setup by processing a JSON-based configuration file stored in DynamoDB. The automated workflow includes:
- Creating the Transfer Family user role.
- Setting up IAM policies.
- Configuring JCL and SSH keys for mainframe connectivity.

Once enabled, Transfer Family handles data transfer execution seamlessly.

#### Conversion Engine Enablement via DICE
For data conversion, DICE provisions essential infrastructure:
- SNS topics for notifications.
- EventBridge rules (if scheduled execution is required).
- SQS triggers for job execution.

---

### Conversion Engine Execution Flow
The Conversion Engine processes files by converting raw data into optimized Parquet format. The workflow involves:

1. SQS event notification triggers a Lambda function upon file arrival in S3.
2. Lambda function launches an EC2 instance for processing.
3. The EC2 instance runs Python (PySpark) and Shell scripts to process data.
4. The converted files are moved to the destination S3 bucket.

#### Mainframe Data Flow
- Mainframe transfers data via Transfer Family.
- A trigger file (index file) is generated, listing transferred files.
- The trigger file is sent to a specific S3 bucket.
- An S3 event notification sends this trigger to an SQS queue.
- SQS triggers the Conversion Engine to process the listed files.

---

### Processing & Optimization Strategies

#### SQS Event Processing
- The ILM team is updating AWS to ensure that SQS events trigger either EC2 instances or Lambda functions as needed.
- The triggered EC2 instance runs batch processing tasks and terminates post-execution.

#### File Splitting & Parquet Conversion
- Default file split size before conversion is **3GB**.
- The split size can be adjusted based on historical performance and consumption tool compatibility.
- Naming conventions follow a sequence-based approach:
  - Example: `file1_001.parquet`, `file1_002.parquet`.

#### Handling Source File Interpretations
- Ongoing collaboration with the mainframe team to define encoding, delimiters, and data structures.
- JSON format is used to specify file interpretation parameters such as encoding and data types.

#### Data Processing Considerations
- Schema mapping for correct format handling (date, headers, empty characters, etc.).
- Performance tuning for optimal instance selection and file handling.
- Mainframe team can send multiple trigger files for parallel processing.

#### File Types & Schema Handling
- Supports `.out` and `.out.gz` file formats.
- Schema mappings between mainframe data types and Spark data structures are documented.

#### S3 Folder Structure & Partitioning
- Current raw bucket mirrors the landing bucket structure.
- Future partitioning will align folder structures with attributes like **state and termination dates** for efficient querying.

---

### Error Handling & Notifications

#### Conversion Engine Failure Handling
- Failed files are excluded from raw storage.
- SNS notifications report success or failure for each trigger file.
- Mainframe team is responsible for resubmitting failed files.

#### Trigger File Reprocessing
- Failed files can be resent individually or as part of a new trigger file.
- Automated retries were considered but posed downstream issues, leading to a preference for manual reprocessing.

#### Notifications & Logging
- Success/failure notifications provide insights into:
  - Source vs. processed file counts.
  - Schema validation and compression details.
  - Failure reasons with detailed logs.
- Currently, notifications are sent to individual emails but will eventually be consolidated into a shared mailbox.

#### Handling Blank Files
- Mainframe may send empty files, requiring validation on processing impact.
- Tests will determine if blank files generate errors or empty Parquet outputs.
- The team needs to decide whether to filter out blank files before conversion.

---

### Conclusion
This document outlines the role of AWS Transfer Family and the Conversion Engine in facilitating seamless data migration from mainframes to AWS. Key considerations include authentication, file integrity, monitoring, and error handling. Future enhancements aim to optimize file processing strategies and improve monitoring efficiency through centralized notification systems.

#### Next Steps
- **Finalize the folder structure** for raw and processed data.
- **Optimize batch processing strategies** to improve efficiency.
- **Enhance monitoring** using centralized dashboards for better visibility into failures.
- **Standardize retry mechanisms** to improve reliability of failed file processing.

