import os
from pyspark.sql import SparkSession

spark = SparkSession.builder.appName("DataValidation").getOrCreate()

# Define base paths for metadata and parquet files
metadata_base_dir = "/var/s3fs/OT800/ETBG"
parquet_base_dir = "/var/s3fs_t/OT800/ETBG"

# Function to read the metadata and extract column names and data types
def read_metadata(metadata_path):
    metadata_df = pd.read_csv(metadata_path, skiprows=2)  # Skipping first 2 lines as per your input
    column_data = {}
    
    # Extract relevant columns from metadata
    for index, row in metadata_df.iterrows():
        column_data[row['COL_NAME']] = row['DATA_TYPE']
    
    return column_data

# Function to process all directories dynamically
def process_directories(metadata_base_dir, parquet_base_dir):
    # Iterate over subdirectories dynamically for metadata files
    for root, dirs, files in os.walk(metadata_base_dir):
        # Look for any file ending with .metadata
        metadata_files = [f for f in files if f.endswith(".metadata")]
        
        for metadata_file in metadata_files:
            metadata_path = os.path.join(root, metadata_file)
            
            # Read metadata schema
            metadata_columns = read_metadata(metadata_path)

            # Look for .parquet files in corresponding parquet_base_dir subdirectory
            relative_dir = os.path.relpath(root, metadata_base_dir)  # Get relative directory path
            parquet_dir = os.path.join(parquet_base_dir, relative_dir)  # Construct corresponding parquet directory path
            
            if os.path.exists(parquet_dir):
                parquet_files = [f for f in os.listdir(parquet_dir) if f.endswith(".parquet")]
                for parquet_file in parquet_files:
                    parquet_path = os.path.join(parquet_dir, parquet_file)
                    df_parquet = spark.read.parquet(parquet_path)

                    # Extract schema from parquet
                    parquet_schema = df_parquet.schema
                    parquet_columns = [field.name for field in parquet_schema]
                    parquet_types = [field.dataType.simpleString() for field in parquet_schema]

                    # Store schema info
                    parquet_schema_dict = dict(zip(parquet_columns, parquet_types))

                    # Compare parquet schema with metadata schema
                    print(f"Comparing schema for {parquet_path}:")

                    for col_name, dtype in parquet_schema_dict.items():
                        # Check if column exists in metadata
                        if col_name in metadata_columns:
                            metadata_dtype = metadata_columns[col_name]
                            if dtype != metadata_dtype:
                                print(f"  Mismatch for column {col_name}: Parquet type is {dtype}, Metadata type is {metadata_dtype}")
                            else:
                                print(f"  Match for column {col_name}: Type is {dtype}")
                        else:
                            print(f"  Column {col_name} not found in metadata!")

                    print("\n" + "-"*50 + "\n")

# Call function to process directories and files
process_directories(metadata_base_dir, parquet_base_dir)
