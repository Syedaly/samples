import os
import pandas as pd
import boto3
import requests
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, count, when

# Initialize Spark session
spark = SparkSession.builder.appName("DataValidation").getOrCreate()

# Define directory pairs (metadata_base_dir and parquet_base_dir)
directory_pairs = [
    ("/var/s3fs/OT800/ETBG", "/var/s3fs_t/OT800/ETBG"),
    ("/var/s3fs/OT800/ETBG1", "/var/s3fs_t/OT800/ETBG1"),
    ("/var/s3fs/OT800/ETBG2", "/var/s3fs_t/OT800/ETBG2")
]

# Function to normalize data types
def normalize_data_type(dtype):
    return "string" if dtype.lower() == "char" else dtype.lower()

# Function to read the metadata and extract column names and data types
def read_metadata(metadata_path):
    metadata_df = pd.read_csv(metadata_path, skiprows=2, header=None, usecols=[1, 2], names=["COL_NAME", "DATA_TYPE"])
    metadata_df["DATA_TYPE"] = metadata_df["DATA_TYPE"].apply(normalize_data_type)
    return dict(zip(metadata_df["COL_NAME"], metadata_df["DATA_TYPE"]))

# Function to check for null values in a DataFrame
def check_null_values(df, file_path):
    null_counts = df.select([count(when(col(c).isNull(), c)).alias(c) for c in df.columns])
    for col_name, null_count in null_counts.collect()[0].asDict().items():
        if null_count > 0:
            print(f"Column {col_name} has {null_count} null values in {file_path}.")

# Function to check for duplicate rows in a DataFrame
def check_duplicates(df, file_path):
    duplicate_count = df.count() - df.dropDuplicates().count()
    if duplicate_count > 0:
        print(f"Found {duplicate_count} duplicate rows in {file_path}.")

# Function to process a single directory pair
def process_directory_pair(metadata_base_dir, parquet_base_dir):
    if not os.path.exists(metadata_base_dir):
        print(f"Metadata directory does not exist: {metadata_base_dir}")
        return

    if not os.path.exists(parquet_base_dir):
        print(f"Parquet directory does not exist: {parquet_base_dir}")
        return

    metadata_files = [f for f in os.listdir(metadata_base_dir) if f.endswith(".metadata")]
    parquet_files = [f for f in os.listdir(parquet_base_dir) if f.endswith(".parquet")]

    if not metadata_files:
        print(f"No metadata files found in {metadata_base_dir}")
        return

    if not parquet_files:
        print(f"No parquet files found in {parquet_base_dir}")
        return

    for metadata_file in metadata_files:
        metadata_path = os.path.join(metadata_base_dir, metadata_file)
        try:
            metadata_columns = read_metadata(metadata_path)
            for parquet_file in parquet_files:
                parquet_path = os.path.join(parquet_base_dir, parquet_file)
                try:
                    df_parquet = spark.read.parquet(parquet_path)
                    parquet_schema = df_parquet.schema
                    parquet_columns = {field.name: field.dataType.simpleString().lower() for field in parquet_schema}

                    print(f"\nChecking schema for {parquet_path}:")
                    for col_name, dtype in parquet_columns.items():
                        if col_name in metadata_columns:
                            metadata_dtype = metadata_columns[col_name]
                            if dtype != metadata_dtype:
                                print(f"Mismatch: {col_name} (Parquet: {dtype}, Metadata: {metadata_dtype})")
                            else:
                                print(f"Match: {col_name} -> {dtype}")
                        else:
                            print(f"Extra column in Parquet: {col_name}")

                    check_null_values(df_parquet, parquet_path)
                    check_duplicates(df_parquet, parquet_path)

                except Exception as e:
                    print(f"Error reading parquet file {parquet_path}: {e}")

        except Exception as e:
            print(f"Error reading metadata file {metadata_path}: {e}")

# Process each directory pair
for metadata_base_dir, parquet_base_dir in directory_pairs:
    print(f"\nProcessing:\nMetadata: {metadata_base_dir}\nParquet: {parquet_base_dir}")
    process_directory_pair(metadata_base_dir, parquet_base_dir)

# Stop Spark session
spark.stop()
