KT Sessions from Accenture - Terraform and IAM Part1 -20241112

0:02
Can you guys see?

0:05
Yeah, go ahead, Sir.

0:07
Yes, we can see your screen.

0:09
Awesome.

0:09
OK, so for the purposes of this call, I'll just be using the report test account.

0:16
The Redshift serverless clusters are only in the reporting accounts.

0:19
They're not in any other accounts.

0:21
That's why you'll only see this role being available in the reporting account.

0:26
So I'll just log into here.

0:30
Oh well, I guess I waited too long.

0:31
Let me refresh this.

0:48
All right, so this documentation goes over the initial setup.

0:54
However, like these steps, you know, can also help give insight into the different configurations that need to be set on your Redshift serverless cluster.

1:03
I won't go through all of them for the sake of time.

1:05
I just want to point out a, a few things and then, you know, everyone can ask questions as they, as they seek that.

1:15
So the, the first thing I would say is, is that the networking part.

1:20
So currently we just have a security group.

1:24
I think we might be using the default security group and there should be no inbound communication because we're not connecting to this database from like over the data plane.

1:38
We're connecting to it for over the control plane.

1:40
And what that means is, is AWS handling all the inbound connectivity.

1:45
So you won't see any inbound connections here unless if you guys significantly change how you're administering like formation and everything else.

1:55
As far as outbound, the only thing it needs outbound to is what data sources it's connecting to.

2:00
And for us, that's Glue and S3 and like formation.

2:05
So if you guys added an additional data source like some other database that you wanted Redshift to be able to pull data from, that rule would be added to the security group that's attached to the Redshift serverless cluster.

2:16
And so whenever we look at that, what I mean by that is I think it's on the work group, not the namespace.

2:34
Yep.

2:34
So whenever you see here, it's it would be the security group here.

2:40
All right, any questions on like the networking part before I move forward?

2:49
OK.

2:49
Hey, Jacob.

2:50
Hey, Jacob.

2:51
Yeah, Jacob.

2:52
So like we will be requiring inbound rules and like I'm working with cloud team, we need inbound connections hitting from outside like like AWS network to serverless.

3:09
So that configuration we need to update here or it's like a separate VPC update.

3:15
So why are why are we adding inbound connectivity?

3:20
So there are some like like some teams, some users which are using different tools like DB Viewer, Sequel Lite or something like that, even Power BI to to get access to serverless.

3:34
So in that case, so, so for your for the permission model to work with the data lake, you have to connect to Redshift serverless either with Federated login, which is via roles or via an IAM user.

3:51
So neither one of those require inbound rules.

3:55
Those are the only two methods.

3:57
So everything else that that you will find online will not support the Lake formation permission model.

4:05
So if you do that and if you somehow get it to work, you're probably going to be creating a security incident because it you have to use AWS permissions to to pass into your serverless cluster.

4:22
So what you are saying is like the Federated user and the like, I am user support inbound rules or do not support inbound rules.

4:32
They don't support there.

4:34
There's no, there should be no inbound network connectivity.

4:39
So the reason for that is lake formation permissions.

4:43
He's talking about Sushil.

4:46
OK, OK, got it.

4:47
So you can't, you can't connect, you can't connect to Redshift with a like a, a database user like you do with, you know, Oracle or MySQL.

4:58
It, it doesn't work that way.

4:59
You have to connect to it with a Federated login, meaning either an IAM role or an IAM user.

5:08
If you don't do that, your whole permission model breaks down.

5:12
And so that's why I'm saying for this like you should, if someone says I need an inbound rule on my security group, that should raise all kinds of eyebrows.

5:24
Everyone should, you know, it should just it should sound weird.

5:27
It should make everyone uncomfortable.

5:28
Like adding inbound rules on this should just not be a thing.

5:34
OK.

5:35
No, I just, I got it like just want to make things some clear.

5:39
And one more question like like you have created the Federated user and all, did you created any admin user for this serverless like which for which you have password or some?

5:54
Yeah, that's a great question.

5:56
So the the admin, so it's created in this step.

6:01
So if we go to the Redshift serverless configuration, I think it's maybe it's on the workgroup or the namespace, but it's, it's automatically managed by AWS.

6:16
So if we look at here the admin credentials, you'll see that it's already encrypted with the with the data like KMS key and it's saved in secrets manager and it's automatically rotated by AWS as well.

6:33
So if we go into Secrets Manager, I think, yeah, I can see it.

6:40
So you'll see here, it's Redshift exclamation marks.

6:43
So these exclamation marks, these mean that these are managed by a service.

6:47
So for example, if I open this up, yeah, see here, you see this nice little info thing here, it was created by Redshift and it's managed by Redshift.

6:55
See, it says you, you're not able to modify the secret value.

7:00
So this is completely managed by AWS.

7:02
So the rotation is managed by AWS and everything else.

7:07
OK, Yeah.

7:08
So this is the only administrative.

7:10
Yeah, go ahead.

7:11
Yeah, yeah, no, no, this, this is what I was looking for.

7:14
Like how this is created, because this is quite important for us.

7:19
Yeah.

7:20
And so one other thing I would say is so to connect with that credential, you won't be able to query any data because you know, for a least privileged model, this administrator should not be able to query data like they don't have a reason to.

7:35
But it does need to do some administrative things.

7:38
So like if we go to our wiki, you'll see that, for example, if you create a new IAM user, you'll have to run these steps.

7:47
Or if you create a new IAM role that you want to use with your like formation setup, you'll have to run these commands as an admin.

7:55
And so those steps are here, but I can just show it real quick as well.

8:00
You just go to the the connection, you click on edit connection and then here on the AWS Secrets manager, you specify the the secret that corresponds to your, your database name.

8:15
So data like Dash reporting, and then you just click save and then you'll be connecting with those credentials.

8:20
So those those instructions are right here, but there's basically nothing other than these two steps that you should be doing with an administrator, unless if you're trying to configure some Redshift serverless like functionality, you would log in with this user.

8:35
But any type of permissions like if a role needs additional access to something, it wouldn't be here.

8:41
So I would really caution you guys from like getting getting creative with like database level permissions because by default it keeps all of the data isolated between user sessions.

9:00
But if you start creating like local tables and shared tables, the chances of you having a security issue increase significantly.

9:10
So I would really caution you guys from using this admin role other than to like change server like database level settings or granting these very specific permissions here.

9:28
So but these roles must be in the same account, right Jacob, like the the usage.

9:34
That's right.

9:35
Yeah, they don't they don't support crossed account.

9:38
Yeah.

9:39
So, you know, the role has to be in, you know, test like the test account and it's I mean, it's gotta be in the same account as your cluster, the serverless.

9:50
OK, Yep.

9:53
I mean, if you needed to do some kind of cross account tomorrow, you could you could have like a trust relationship and assume a different role, But I mean that would like I wouldn't really recommend it.

10:02
I would keep your account boundaries in place.

10:08
I see.

10:08
OK, but but the the the AWS data catalog has the cross account permissions.

10:15
I mean, which is indeed reading from the data governance icon, right, the catalog.

10:19
But that role we are calling from the server list and able to access the data from the server list, right?

10:26
Oh, I see what you're saying.

10:27
That's a great question.

10:29
So what happens is, is that the server list cluster doesn't have any permissions itself.

10:36
So you'll see here this is intentionally left blank.

10:39
So if some if anyone logs in and you see a role here like you should be worried because that shouldn't be here.

10:46
So this is intentionally left blank.

10:49
And the reason why is because whenever you log in with a the Federated login.

10:56
So whenever I click this button, what it actually does is is so right now I'm logged in as off Redshift admin.

11:04
AWS generates a connection string based off of this role here and it passes it to Redshift.

11:13
And then Redshift then uses those exact same credentials to pass them to Glue or sorry, yeah, it's a glue and to late formation and then eventually to S3.

11:25
So it takes care of all of that for you.

11:27
It just, it's like, think of it like a forwarding mechanism for your credentials.

11:33
Like it gets forwarded down through the different layers eyes.

11:39
Does that make sense?

11:41
It does, but, but I, I was just trying to like, you know, understand the, the connectivity between the serverless and the glue catalog that we have, right.

11:51
Yeah.

11:51
So it uses your own permissions, like your own role.

11:56
So like whenever I'm logged in as this role, there's actually three sets of strings that it creates.

12:04
It's like a access key ID, access secret key and like a session token.

12:08
So then AWS does all this it's called SIG V4.

12:12
It, it more or less turns those three strings every single request gets signed.

12:18
And then it like creates basically a a unique off token that's only like valid for a single second.

12:23
And so that off token is what's then passed through the different layers is how it technically works.

12:29
I see.

12:30
So, for example, like I wanted to onboard a different different role, right?

12:38
And a different user, right, different console user, let's say like operational external read role.

12:46
And I wanted to have like, you know, permissions only to the only to the like, you know, the analytical layer, let's say, OK, not to the process.

12:57
So I would create an IM with needed permissions, which will indeed have like, you know, the policies, like you know, which will have the the policies defined towards that glue catalog, right?

13:09
Like if you go to yeah, if you go to console.

13:12
And then like once I create that role, now I have to come to the Redshift and create that connection string, right, that you were showing with that specific role.

13:23
That's right.

13:23
Yeah.

13:24
So more or less the read roles, no matter what level of permissions that they are given, they'll all be identical with the exception of here you see this work group.

13:37
So with Athena you have to get more specific.

13:41
But for if it wasn't for Athena, the roles would actually be completely identical.

13:47
So like all the redshift permissions are all completely identical.

13:51
It uses if you see here, it uses see these like conditional statements.

14:00
So every single policy is exactly the same exactly for the right shift part.

14:05
Yeah.

14:05
The only thing that's different is the stuff for Athena.

14:08
So so Athena, the workgroup, right.

14:11
Is that a mandatory like, you know, thing to like, you know, put put in the policy Jacob like or yeah, that's that's a great question.

14:23
So if you have a different role, it it is mandatory.

14:27
So if you remove this from your policy, so if you just put star here, you've now like given this role, the ability to access all data that anyone else has access to in the data lake.

14:44
OK, So essentially we have to just define a policy how it how it would work on the Etina, right.

14:53
And then now we are using the the the 02 policy to kind of make connection or make connection as in to enable that communication between the server list to the Athena work group, right?

15:05
The the glue catalogue.

15:10
Sorry, I didn't catch your question.

15:11
Can you repeat that?

15:12
Sorry.

15:13
So essentially I have to like, you know, the, the odd data sense read only has all the permissions like how traditional or atypical 18 like no word group would work, right.

15:26
So we have to come with that.

15:27
And then now at the second step, if we are seeing the data sense read only 02 right, 02 version or O2 version.

15:36
So that is where now here, like we, we, we are giving OK, we are giving permissions for our relative server list to access this catalogue from the glue, correct?

15:49
Yeah.

15:50
So that, that that's exactly right.

15:52
Yeah.

15:53
OK.

15:53
So these will be the same for all of it.

15:55
So you, this is like the first step.

15:57
And then once once this role's created in Terraform, we reviewed that I think last week where you have like the role name.

16:06
And then whenever you add that in the Terraform, you specify that the like permission tag combinations that you want it to have access to.

16:13
And the Terraform actually automatically creates the the Athena work groups for you.

16:21
So that's handled automatically.

16:23
We just can't create these roles because that's, you know, cloud services domain.

16:27
OK, I I get it.

16:29
Yeah, yeah.

16:30
And so other than that, the only other thing that has to be done whenever you onboard a new role is these steps.

16:37
So you have to log in and then you have to create this.

16:43
It's a very specific, you have to like run these two commands.

16:47
Yeah, yeah, yeah.

16:48
OK.

16:48
And that's it.

16:52
Yeah.

16:52
Team, these should be the only users that we create here.

16:55
So if it it should start with IAM or IAMR.

16:59
So AMR read the only yeah, IMR is for IAM role.

17:04
And then if it's just IAM, that's an IAM user.

17:07
But it should always start with this.

17:09
Like there should be no like database user that gets created.

17:14
So like if somebody's specifying a password for a user, So if somebody's saying like a password here or something there, that's a problem.

17:22
Like that shouldn't be a thing.

17:24
Yeah, I get it.

17:25
Because because of the lake formation permissions that we have for the roles and the users and stuff like that.

17:32
Yep.

17:34
Question Amar, how does Amar and Shashil, how does this align with if you wanted to connect to the Redshift, you just want to connect the Redshift through a Power BI.

17:44
So, So with this setup, yeah, we have to come up with the IM user shade.

17:50
Yeah, we have to create IM user and then bind that permission to this IM role and that user can access outside once that 5439 port has been resolved.

18:02
But then they can connect.

18:03
OK, yes, one question user, one question to be like, you know, we we need to give permissions on the link formation for that user.

18:12
Yeah, I have one question for Jacob.

18:15
Jacob, one question like you are explicitly saying that these only three commands or all the role or whatever is need to be done every time.

18:24
So As for the naming standard, the first one says it's a read role, right?

18:29
And the last one is also read roll.

18:31
But what where's the read write bucket fall?

18:36
Well, I don't think that you would be writing data to the data like via Redshift.

18:43
If like within the Redshift tables like user want to do a write thing also, is it not possible they can do.

18:52
I mean, sure you can do it, but then that's not like permission enabled.

18:56
So now to add that very simple, you know, functionality, now you have to come up with a whole permission model and access matrix and management structure for that table.

19:10
So sure, you can do that.

19:13
However I would advise against it.

19:16
So all of these three are read only specific, nothing with the AD permissions, correct?

19:23
Correct.

19:23
That's right.

19:24
Yep.

19:25
And sometime like user want to run certain DMVS or so some temp table and all.

19:30
So how that can be accompanied?

19:34
I mean all that's possible.

19:36
I'm just, I'm just saying I would exercise caution if you start to enable that functionality because it it, it'll be very easy because then you're going to have to start granting additional permissions and it's going to be really easy to cause to create a data, a data access problem.

19:55
So all that stuff can be done.

19:57
Yeah, it's definitely possible.

19:59
Just exercise caution whenever you're doing that.

20:02
Yeah.

20:03
Because, you know, we're having one database with a lot of different data in it.

20:07
So, I mean, you know, once you guys are done with your migration, you're going to have hundreds of data sources.

20:12
You're going to have, you know, some very sensitive data here I think as well eventually.

20:17
So, you know, I would just use extreme care, you know, something as simple as that.

20:21
You know, it might seem really simple, but, you know, you just have to evaluate the risk of doing it.

20:25
Is it really worth it?

20:26
Yeah, Yeah, definitely.

20:28
Like we do have a Tier 1 application and we do all those things, but yeah, I just want to make sure, like for these rules.

20:34
So yeah.

20:35
OK.

20:35
Thank you.

20:37
Yeah.

20:40
All right.

20:40
Any other questions on this?

20:47
OK, the only other thing I want to note is on the enhanced VPC routing.

20:55
So with that, what happens is, is that whenever you query data, it's let me see if I can, I mean, you guys have seen it before so I'm not gonna like log in again.

21:09
But you, you guys know whenever you see like all the, the, the databases and stuff and you query the, the tables.

21:14
So what's happening is, is that your query is being routed locally, like privately.

21:20
So if you ever have an issue where it's like taking a long time to load, the 1st place I would look is if something's changed with the endpoints that are deployed in these accounts.

21:33
So that would be my first thing.

21:34
So maybe it's a security group change or ADNS change or you know, something like that.

21:41
So that's what I would check first.

21:43
The errors are really unclear whenever like a problem like that comes up.

21:48
So that's where I would go first.

21:54
That's all I really had to cover with this.

21:57
Everything else is super clear in the document.

22:02
Are there any other questions on this part for the Redshift?

22:14
No, so, so just to just to clarify, right, like these steps has to be carried like, you know, after the deployment, after our telephone deployment.

22:24
And this, this is like an admin setup.

22:27
This is not integrated into our Terraform, correct.

22:31
This is not integrated in a Terraform.

22:33
This is completely independent.

22:35
Yeah.

22:36
So, yeah, Terraform is managing all of our late formation things as well as Athena.

22:41
But Redshift, because you have to login to do some of these steps, it's really, I mean, theoretically it's possible if you spend a whole lot of time automating this with Terraform.

22:51
But I mean, it would be, it's not, it wouldn't be worth it.

22:54
Like I wouldn't.

22:54
Yeah, advise doing it.

22:56
Yeah, Yeah.

22:59
Typically database administration like I, you know, I typically try to keep that, you know, application level configurations that typically try to keep that out of Terraform is what I always recommend to clients.

23:14
OK, yeah, thanks.

23:16
Anything else?

23:19
Nothing from my side on this one.

23:21
Yeah, I like, OK, So how like you have not mentioned like how we do the SSL bindings in the Red Ship server less work groups like if we want to enable the SSL thing so that data at rest and data at motion are being properly secured.

23:41
Do you have any idea or like insight how we can do that to secure the connections?

23:47
Yeah, so that's already all done.

23:49
So at rest is using, I think I showed that under here.

23:56
Yeah.

23:57
So this is data at rest.

23:59
So that's already done.

24:00
So it's all done with your.

24:03
They had a light KMS key.

24:04
Additionally, we have audit logging enabled.

24:07
I don't think I saw any hard requirements on it, but just about every enterprise I see requires this.

24:12
So all of the audit logging is enabled on the cluster.

24:16
And then as far as encryption and transit, that's yeah, that's handled by AWS.

24:21
So the only service that you have to do, just about the only service you have to do anything special on that with is the oldest AWS service, which is S3.

24:30
Everything else, TLS 1.2 is enforced on every AWBS endpoint and there's lots of blogs you can find AWBS universally deployed TLS 1.2 everywhere.

24:42
Like, I don't know, like a year ago or something they've finally found so, So within, within this serverless, like I said, I like, I recall, like I was discussing with Amandeep on this, with serverless, we can have both secured and unsecured connection.

24:59
It's open for both, but right now you are mentioning that it is, it is only secure.

25:06
It's it's it's not.

25:09
There's no inbound connection, so you're having to go through the AWS control plane, which is HTTPS.

25:18
So there's no unsecured connection to AWS other than some like edge cases with S3.

25:28
But if a user, if a user is trying to connect with endpoints where that SSTTPS is there is there any HTTPS?

25:37
Yeah, yeah, it's, it's by default.

25:41
So yeah, I mean, there's there's nowhere where it has HTTP.

25:43
So if somebody can show me where that's at, I'm happy to clarify.

25:46
But there's, there's nothing that, that I know of that I've seen on any ATPS service, like I said, other than S3, they use HTTP.

25:56
So you have to like there's no, and there's no network.

25:59
I mean, there's no inbound connection.

26:01
So there's, there's, I mean, there's no way.

26:04
I mean, there's no inbound rules on your security group.

26:08
So if we look at the security group, there's nothing inbound.

26:11
So there's, you don't even have to worry about that then.

26:14
So the security group doesn't allow anything inbound.

26:17
So like there's nothing to, you know, I mean, there's, there's no traffic going directly to the, the cluster.

26:26
Oh, no, I get it.

26:27
Security group part, I get it.

26:28
But my question was like, if I want to present that, OK, how the SSL binding data at transit is being secured, how I can show that I know that AWS is taking care of it, but from serverless, it's a from the configuration.

26:42
Is there any parameter?

26:44
No, because it's, it's by it's by default.

26:46
So I would encourage you to look at the shared responsibility model.

26:51
So that's just something that AWS manages for you.

26:54
You know, if you were managing this on a database on some server somewhere, sure, you would have to do that SSL binding and importing of certificates.

27:03
That's one of the benefits of working in AWS is that they handle that for you.

27:09
OK.

27:09
Yeah.

27:10
Because that is an important question we have to work on, but yeah, OK, Thank you.

27:15
Yeah, it's it's it's already done.

27:16
That is the answer.

27:17
So you're good to go.

27:21
Thank you, Jacob.

27:23
Sure.

27:23
All right.

27:27
Any any other questions?

27:32
Hey Jacob, can you, I don't know maybe if you have already covered this, but maybe a scenario based things help helps too, right.

27:40
I've been asking the team to go back, check the recording and try to come up with more questions, but no, what's known is what's unknown is unknown, right?

27:49
So maybe can you help if this was already discussed, it's fine.

27:56
But otherwise, could you help me understand or help the team understand like hey, if there is a new rule that needs to be created for a consumer account for a new table, what are the sequence of steps that needs to be done right?

28:13
So OK, first you go double up this in your dev account data form.

28:18
Here is a.

28:18
Here is how you basically create a new one and or add this one.

28:23
This is what you do for the process.

28:25
Late analytics late.

28:26
This is what you do role wise.

28:28
Here you go.

28:29
These are the things that you do in Terraform.

28:31
Come back to the link formation.

28:34
These are the taggings that you would set and then go to the red chef here create those roles.

28:39
Maybe a scenario would help like for example, like if there is a new requirement, let's assume that there are two more tables that are that need to be developed, right?

28:52
And that belongs to.

28:55
Maybe let's just say these are consumer tables and we have to come up with like a new role and like form and and and add permissions accordingly.

29:04
How does that end to end flow look like?

29:08
Maybe a scenario would help.

29:10
Let's just let's just assume like if we have time in 20 minutes, could you show us, help us the team understand?

29:15
Like if I have like two more tables that got added to the day leg and these are like customer tables and known and we need to develop a completely new role.

29:26
These are not operational, but but maybe OK, maybe these two tables needs a separate role, but these two tables also needs to get added to the operations role as well.

29:36
Like how do we do that end to end?

29:38
Like could you help us walk through that scenario?

29:42
Sure.

29:43
Yeah.

29:43
So I think that I've that's already been all covered.

29:46
But Amar, I think that maybe you would have the the greatest visibility on on that.

29:51
Like if, if maybe I missed something.

29:55
So like we do have some other topics we wanna go through as well.

29:58
But if that's something that we're still unclear on, I can definitely go through that again.

30:02
I'm I'm happy to, but like Omar, like wherever you sit on that, like are you comfortable that I've covered all of those things?

30:07
Or, you know, I want, I want to ask the team at least more than more than more than one to see if you guys have a full understanding of what needs to be done.

30:18
If you have other topics continue.

30:20
But I'd like to do that scenario playing things and see like where the team sets.

30:25
Right.

30:27
OK, Yeah, OK.

30:34
You're making up.

30:35
I'm like you're cutting.

30:36
Sorry that Raj, we can't hear you.

30:37
Yeah, it's it's cryptic.

30:42
Yeah.

30:42
So, so maybe I'll, yeah, maybe I'll try to cover the the other topics and then yeah, yeah.

30:48
Continue to go through a scenario.

30:51
That's fine.

30:52
Yeah.

30:52
I just, I also want to make sure that, you know, we don't have too much, you know, because what I've, I've done before is like I create too much recording and then it just becomes a duplicative.

31:01
And then it's like where, where do I go?

31:03
So I'm happy to go through that again, just just let me know.

31:07
But I would encourage the the team to, you know, review the, you know, whenever you're thinking about those scenarios.

31:12
I've always found that the best way to see if you understand it is to try it yourself.

31:16
So I, I would encourage the team and whenever they have those questions to see like are, are, have they been covered is what I would encourage because then you'll start to understand where are the different pieces of information.

31:26
I did cover where they're at.

31:27
So that way, you know, once I'm not around anymore, you guys will be able to be self-sufficient.

31:34
Yeah.

31:35
Yeah, that's fine.

31:35
Continue with the other thing.

31:37
I'll follow up with the team internally.

31:38
And if, if needed, it's not gonna be a hard job for you too, right?

31:41
It's only a 15 minute.

31:43
Probably.

31:43
It takes you 15 minutes to chill end to end.

31:46
Good.

31:46
Continue, Jacob.

31:48
OK, Sure.

31:49
OK.

31:50
So the next topic I wanted to to touch on is related to Terraform.

31:55
So I know that we went through more how to use Terraform already and there were maybe like some very basic K TS on it.

32:03
However, I wanted to explain a, a little bit of the nuances of Terraform and Terraform Enterprise.

32:11
So that way the, the group would, you know, know some of the gotchas, know some of the things that, you know, I'm just trying to instill some of my experience, you know, because I've used Terraform Enterprise and Terraform Cloud quite a bit.

32:26
So I'll start with, you know, I'll kind of go, what I'm going to do is I'm going to go through the journey of, you know, going from, you know, starting to do a Terraform init, a Terraform apply.

32:37
And where does that go?

32:39
Like, how does that work?

32:41
Because what I found is, is that whenever you, whenever people first start using terraform there, it's very confusing.

32:47
You know, how does this work?

32:48
What's it doing?

32:50
You know, something's not working.

32:51
How do I fix it?

32:53
So I'm going to be walking through the the details of that.

32:57
So that way, you know, people will be able to troubleshoot any potential issues that may come up.

33:05
So the first thing I would touch on is this terraform block.

33:09
So previously PSC, maybe you guys had, you know, before you had tear from Enterprise, maybe you guys had your state files stored in S3 or something else.

33:18
So if that was how it go ahead.

33:22
I think you're sharing a different screen maybe.

33:24
Uh oh, yeah.

33:26
Thanks for stop stopping me.

33:28
I was trying to share VS Code.

33:29
Is that not what I was?

33:30
Yeah, no, no, you're sharing your the week the ratio of page that's not super helpful.

33:41
Thanks for stopping me.

33:45
All right, can you guys see it now?

33:47
Yes, OK.

33:49
All right.

33:49
So it's this terraform block.

33:50
So that's what I was talking about that you guys couldn't see at all.

33:54
So this terraform block is the starting point.

33:56
So this is saying where am I going to connect to?

34:00
And that's, that's basically it.

34:04
Yeah.

34:04
So this this within the Terraform, there's this cloud block.

34:08
So instead of using Terraform Cloud, you guys are using Terraform Enterprise.

34:11
So you specify your host name.

34:13
This is your organization with workspaces.

34:16
You'll notice documentation online has a lot of other options.

34:19
I've never used any other option other than just specifying the name directly.

34:23
And I would encourage you guys to do the same.

34:25
Some of the other functionality, it just becomes more cumbersome and it just leads to mistakes.

34:29
So I would leave, I would configure it similar to this for all of your connections.

34:36
Additionally, this isn't a require like a thing you have to do.

34:39
But I always find that setting the required version block here just helps developers make sure that they you don't have the right versions installed locally.

34:49
Just it's, it's an experience thing I've seen where developers have been like very outdated versions installed and it just creates issues during deployments.

34:58
So, you know, if you have to do an upgrade of your Terraform workspace version, which isn't very often, but if you opt to do it, you know, there's lots of documentation online on how to do that.

35:13
But yeah, I just recommend keeping this In Sync with the version of the workspace.

35:18
So whenever I say the version of the workspace, what I mean is, is each workspace has a version that is specified, like a Terraform version, which can be changed and upgraded.

35:30
And depending on the, you know, the version that you're upgrading to, there's special documentation on, oh, you know, you're upgrading to this version.

35:38
There's these problems, you know, upgrading from version A to version B.

35:42
You have to do these steps.

35:43
Like Hashi Corp's very good about documenting those kind of things.

35:46
But this is what I'm talking about, this version right here.

35:50
And that can be changed, you know, just by logging in and you can, you know, select, you know, these versions.

35:57
I I also recommend pinning of this is called pinning, selecting a specific version.

36:02
Some people will say to do these like automatically updating flags.

36:07
I found that it, you know, it's less maintenance to do this.

36:11
However, I found that there can sometimes be inconsistencies and it really gets, whenever you do this, it really gets into the nuances of how Terraform does upgrading, which very few people understand.

36:23
Like I don't, I don't even really understand that 100%.

36:27
I've never encountered anyone that does fully understand it.

36:29
So I'd recommend not doing this just again, I've just had issues where these things are out of sync and it's just very unclear to users why they're having an issue in production.

36:40
Yeah, so it used to be the situation we used to just like update the build servers with the latest version, like, I mean, the run that command, update that, update the build server agent.

36:54
Yeah, it was similar.

36:55
Yep, exactly.

36:58
Yeah.

36:59
And we removed that, thankfully.

37:01
Well, actually I think the agent's still there cuz they still have a problem with the TFE servers.

37:06
But yeah, but hopefully they'll get that resolved and you guys can get those agents out of there completely.

37:12
OK, All right, So what happens is is so you, so here we specify the host name, the organization, and the workspace.

37:19
So let's just say for prod.

37:20
So we go, I'm just gonna show you like how it works.

37:22
So whenever you do a terraform in it and a terraform apply, what's happening is, is that it uses a token.

37:32
So like locally you have a token that the first time you do a Terraform, you know any action, it'll say you must log in.

37:40
And your token is used to authenticate from your local environment to Terraform Cloud.

37:47
So you'll notice here in my account settings, you'll see tokens.

37:51
There's a token that's set here.

37:56
And this is how I authenticate locally to Terraform Cloud.

38:02
Whenever, once that's done it, because you've specified this workspace, it then goes here.

38:13
So these are where your workspace variables are defined.

38:17
So we have this, it's called a variable set.

38:21
And these are an administrative thing.

38:25
So that's why I keep these and I use variable sets because a regular developer doesn't have access to change these, but developers do have access to change these and these override these down here.

38:37
So that's why I do it this way.

38:39
It's just from experience, I've always found that like AWBS credentials are best, you know, for the, the, the ones that are typically used are best saved here.

38:49
And then this also enables you to be able to like if there's a break glass situation or something, I don't know what it would be, but for you guys at least.

38:57
But yeah, you can add in, you know, variables here where you can, you know, override these values by specifying them here.

39:04
And developers can do that without administrative rights, but it uses these to initially connect.

39:11
So here, you know, forgetting that we have all this other stuff down here, though, the Terraform Cloud Run now has those IM credentials in its environment variables.

39:26
So just like whenever you have Linux, you know, you have like the export command or whatever it is that sets the environment variable in Linux.

39:32
It's the same thing here.

39:32
It, it opens up, there's like a process that starts and it literally runs a whole bunch of like set environment variables.

39:39
It it also sets other environment variables that you can't change like this TFC run ID, TFC workspace name.

39:45
These are done by Hashicorp does these for you.

39:48
You can't change those, but those are all set in your environment.

39:55
OK, so that user, so this I am user, has specific permissions on what it's able to do and it's a best practice.

40:05
You won't find this documented anywhere, it's not on any online documentation.

40:10
But I've done this many times and it's always best practice to not have these credentials be the ones that can directly modify your infrastructure, because sometimes these things get leaked.

40:23
And the first thing hackers try to do is list as 3 buckets.

40:28
And they, they, there's a few things that they try to do.

40:30
And so if you give these credentials direct access to do something, it increases your risk significantly because the, the permissions that this user has are, are very broad.

40:43
So this user is only able to assume.

40:46
So if we log in, I think it's in the data.

40:51
I'll just show you guys real quick, yeah.

41:31
So you'll see that this user only has very specific permissions.

41:35
It's only able to assume role and set source identity on and these are very specific resources that are put here.

41:43
So this is all it can do.

41:45
So if these credentials get compromised, a bad actor would have to know a lot of information like they would have to have this exact they are in, like they would have to have a lot of stuff in order to actually get into your network.

41:55
So that's why I always separate out and have another layer in between these.

42:01
Yeah, so, so yeah, go ahead.

42:03
So I think that that would be helpful for the team, Jacob.

42:06
So if, if you just go to that policy, right, I think we have assumed 3 roles there and three roles are from three different accounts.

42:14
If you, if you see there right, like 11-12 hundred inlines.

42:17
Yeah.

42:18
So these are all production accounts for data governance and data reporting and also the data actually producer.

42:24
So with one user, essentially we are controlling 3 accounts within the production, right?

42:31
That's right.

42:34
Yeah.

42:35
And just like this, PST is not super mature on their automations quite yet.

42:42
Like the Terraform, I know that it's an objective to get more things under it, but you know, if you're changing this role, just keep in mind where this or sorry, this user.

42:52
Keep in mind where this user is being used at.

42:54
So like this user is being used in a in prod workspaces.

42:59
So if you like, add in a an account number, that's a QA account number.

43:06
Well, now it's very confusing to your developers whenever because this, this user has access to the has access to a FQA account, but it's only supposed to have access to prod accounts and vice versa.

43:20
So you know, these kind of things just makes it make sure whenever you're changing stuff like this that you really think through, you know, where this is being used at and what it's doing.

43:30
Because I've seen in the past with my other clients, people like, you know, something's not working and they start to just add stars and things like that.

43:40
And then, you know, if there's ever an issue, typically it's someone that just makes a mistake.

43:44
But you know, I've seen before where you have users that aren't supposed to have access to prod, but yet in dev they gave access to prod accidentally and then there was a whole security incident related to that.

43:56
So just be careful changing these kind of things and make sure you're considering implications.

44:05
The same thing on the rolls itself.

44:07
So this you know, this user is only able to assume these roles and then vice versa, there's almost like a handshake.

44:17
So the roles, what says what?

44:20
Who can assume these roles is in the trust relationship.

44:23
And you'll see here that only that one IM user is able to assume the role.

44:33
And so this will bring me to my next point.

44:34
So additionally, in addition to that, you know, separation of having the the user and the role.

44:41
Additionally, it's always, it's also best practice to have what's called an external ID.

44:47
The people think of it as a password, but it's not a password.

44:51
It's more like a, an API key is the way that I would I would see it.

44:56
API keys typically aren't passwords.

44:57
They're more like identifying you are who you say you are.

45:02
So this here, I always make it where it's descriptive of what it is that you're assuming.

45:07
So that way whenever you're setting up your Terraform configuration, you know, if you're just pasting in some account number, you know, I don't know by looking at this account number if it's production or dev or test, I don't know.

45:20
However, by this external ID, it's super clear this is a product count.

45:25
So always put that there just as a best practice.

45:29
I always make sure that this is also a random string.

45:31
So that way people can't just change this, like come here and just change this and and just change this.

45:37
Like I always make sure that people are logging in and actually validating what it is that they're using.

45:44
And so that's why I always include like a random string at the end to make sure that people are, you know, doing their due diligence and looking up their information.

45:54
Additionally, I also add in the source identity.

45:58
This is more for purposes of tracking in Cloud Trail.

46:04
So a nice thing by adding this, which you won't see documented anywhere else, this is just something I've learned along the way, is using this source identity, every single action you do in Cloud Trail will now include this string.

46:19
So automatically every time that Terraform does anything, you can actually search in Cloud Trail for data engineering.

46:27
And then, you know, depending on your search string, you can actually search for the workspace name.

46:33
And additionally, you could also search for the specific run.

46:37
So let's just say that there was a problem and you guys noticed that, oh, you know, this bucket was deleted or something like that.

46:45
In Cloud Trail, if you see a, you know, a run I, you can literally come the run ID that was used.

46:56
I saw this in cloud trail.

46:57
It also in cloud trail, it'll have the workspace name and the org name because of how this is forced to be configured here.

47:03
And then you can see exactly who did it, why it happened, things like that.

47:07
That's all done for you and that's it.

47:13
As far As for the Terraform configurations, this is how you, you know, specify, you know where you're actually going to log into to manage your infrastructure.

47:29
There's a few more things that I'll go over as far as I know that we're at time as far as how Terraform Enterprise works, I think would be good for the group to know, but are there any questions about how like the AWS integrations work with Terraform Enterprise?

47:51
No, I think I think this is super helpful, Jacob, thanks for calling this.

47:56
Sure.

47:59
OK, so, so just so one question.

48:03
So in the case of state file, if, if any of my state files are corrupted, right, Jacob So other than other than destroying and recreating like, you know, the planning replanning my terraform, like, you know, are there any steps like we can just like, you know, backward to the, the, the, the version -1 state file, You know, it really depends on what it is, Amar, You know, whenever you start having drift between your real infrastructure and the state file, it gets really tricky.

48:38
Like I even still have trouble sometimes undoing it.

48:42
I mean, I, I would say I'm in a, you know, an expert user whenever it comes to Terraform and I've spent half a day trying to untangle these things.

48:52
So it's always important to test things in lower environments is what I would say.

48:57
So anything that you're going to do, whether it may seem simple or not, do it that exact change in lower environment 1st and then do it in a higher environment.

49:06
So you can always just destroy it and then start over if you need to.

49:17
That's a good question.

49:18
Anything else?

49:25
All right, well, I'll go ahead and end the recording.

49:29
Yeah, Jacob, we want to talk about the dependency on the radio set up.

49:34
We've needed some assistance from the cloud.
